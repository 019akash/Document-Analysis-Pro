# -*- coding: utf-8 -*-
"""documentchecker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jBUnKcmmTRfleEJbA3wDrmIjbtBAzEGS
"""

# First install required packages
import sys
import subprocess

# Function to install packages
def install_package(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# Install required packages first
print("Installing required packages...")
install_package("gradio")
install_package("nltk")
install_package("transformers")
install_package("PyPDF2")
install_package("bitsandbytes>=0.39.0")  # Make sure we have the latest version
install_package("accelerate>=0.20.0")    # For better device loading

# Now import the packages
import os
import re
import uuid
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
import gc  # For garbage collection
import time  # For timing operations
import gradio as gr
from pathlib import Path
import io

# Import PDF reading library
try:
    from PyPDF2 import PdfReader
    PDF_LIBRARY_AVAILABLE = True
    print("PyPDF2 library successfully imported")
except ImportError:
    print("PyPDF2 library not found. Installing PyPDF2...")
    install_package("PyPDF2")
    from PyPDF2 import PdfReader
    PDF_LIBRARY_AVAILABLE = True

# Download NLTK resources
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
    print("NLTK resources already available")
except LookupError:
    print("Downloading NLTK resources...")
    nltk.download('punkt')
    nltk.download('stopwords')

# Create upload directory
UPLOAD_FOLDER = Path("uploads")
UPLOAD_FOLDER.mkdir(exist_ok=True)

# Global variables to hold the model and tokenizer
model = None
tokenizer = None

# Initialize the Hugging Face model - USING NEW GRANITE INTEGRATION
def init_model():
    global model, tokenizer

    try:
        # Using Granite 3.3 2B model as specified in the new code
        print("Loading Granite 3.3 2B model with optimized integration...")
        model_path = "ibm-granite/granite-3.3-2b-instruct"

        # Check if CUDA is available
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {device}")

        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)

        # Load model with optimizations based on available hardware
        if device == "cuda":
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                device_map=device,
                torch_dtype=torch.bfloat16,  # Using bfloat16 as in the new code
            )
        else:
            # Fallback to 8-bit quantization for CPU to reduce memory usage
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                load_in_8bit=True,
                device_map="auto",
                low_cpu_mem_usage=True,
            )

        print("Model and tokenizer loaded successfully with memory optimizations!")
        return True
    except Exception as e:
        print(f"Error loading model: {str(e)}")
        # Suggest using smaller model if loading fails
        print("Suggestion: Try using a smaller model like 'distilgpt2' if memory issues persist.")
        return False

# Helper functions
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in {'txt', 'pdf', 'doc', 'docx'}

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]
    # Join tokens back to text
    return ' '.join(filtered_tokens)

def extract_text_from_file(file):
    """
    Extract text from various file formats including PDF.
    Currently supports: .txt and .pdf files
    """
    try:
        # Get file extension
        file_extension = os.path.splitext(file.name)[1].lower()

        # For Gradio, we need to get the file path
        file_path = file.name

        # Handle different file types
        if file_extension == '.txt':
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()

        elif file_extension == '.pdf':
            if not PDF_LIBRARY_AVAILABLE:
                return "PDF extraction requires PyPDF2 library."

            # Read PDF with PyPDF2
            with open(file_path, 'rb') as f:
                pdf_reader = PdfReader(f)
                text = ""
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text() + "\n"

            return text

        elif file_extension in ['.doc', '.docx']:
            # This is a placeholder - would need additional libraries
            # like python-docx for proper implementation
            return "DOC/DOCX extraction not fully implemented in this demo version."

        else:
            return f"Text extraction from {file.name} file type is not supported in this demo."

    except Exception as e:
        return f"Error extracting text: {str(e)}"

# MODIFIED - Generate text function to only show response (no thinking)
def generate_text(prompt, max_tokens):
    """Generate text using the Granite model with chat template - OPTIMIZED"""
    global model, tokenizer

    # Check if model is loaded
    if model is None or tokenizer is None:
        success = init_model()
        if not success:
            return "Failed to initialize the language model."

    try:
        # Clear CUDA cache before generation
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # Create conversation format for the model
        conv = [{"role": "user", "content": prompt}]

        # Get device
        device = next(model.parameters()).device

        # Apply chat template - MODIFIED to disable thinking mode
        input_ids = tokenizer.apply_chat_template(
            conv,
            return_tensors="pt",
            thinking=False,  # CHANGED FROM TRUE TO FALSE to hide thinking output
            return_dict=True,
            add_generation_prompt=True
        ).to(device)

        # Set seed for reproducibility
        set_seed(42)

        # Generate text
        with torch.no_grad():  # Important to save memory
            output = model.generate(
                **input_ids,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.9,  # Higher temperature for faster generation
                top_p=0.92,
                top_k=50,
            )

        # Decode output
        prediction = tokenizer.decode(
            output[0, input_ids["input_ids"].shape[1]:],
            skip_special_tokens=True
        )

        # Clean up
        del output
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return prediction.strip()

    except Exception as e:
        print(f"Error generating text: {str(e)}")
        return f"Error generating response: {str(e)}"

# Simplified functions with fallbacks for when the LLM times out

def compare_documents_with_ai(text1, text2):
    """Compare documents with fallback to simpler methods if AI fails"""
    # Further reduce text size for T4 GPU constraints
    text1_limited = text1[:5000]  # Drastically reduced from 1000 to 500
    text2_limited = text2[:5000]

    prompt = f"""Compare these two document segments and determine similarity percentage (0-100):

    Document 1: {text1_limited}

    Document 2: {text2_limited}

    Respond with only the similarity percentage:"""

    try:
        # First try with AI
        response = generate_text(prompt, max_tokens=500)  # Reduced tokens drastically

        # Extract percentage from response
        percentage_match = re.search(r'(\d+(\.\d+)?)%?', response)
        if percentage_match:
            return float(percentage_match.group(1))

        # If AI didn't return a valid percentage, fall back to simple comparison
        return fallback_similarity_check(text1_limited, text2_limited)

    except Exception as e:
        print(f"AI comparison failed, using fallback: {str(e)}")
        return fallback_similarity_check(text1_limited, text2_limited)

def fallback_similarity_check(text1, text2):
    """Simple token-based similarity when AI fails"""
    try:
        # Preprocess both texts
        text1_processed = preprocess_text(text1)
        text2_processed = preprocess_text(text2)

        # Tokenize
        tokens1 = set(text1_processed.split())
        tokens2 = set(text2_processed.split())

        # Find common tokens
        common_tokens = tokens1.intersection(tokens2)

        # Calculate Jaccard similarity
        if not tokens1 or not tokens2:
            return 0.0

        similarity = len(common_tokens) / len(tokens1.union(tokens2)) * 100
        return similarity

    except Exception as e:
        print(f"Fallback similarity check failed: {str(e)}")
        return 50.0  # Default value

def summarize_text(text):
    """Summarize text with fallback if AI times out"""
    # Limit to 500 characters for better performance on T4
    text_limited = text[:5000]

    prompt = f"""Summarize this text in a concise manner in 10 sentences:

{text_limited}

Summary:"""

    try:
        summary = generate_text(prompt, max_tokens=3000)  # Reduced max tokens
        if len(summary.strip()) < 10:  # If summary is too short, use fallback
            return fallback_summarize(text_limited)
        return summary
    except Exception as e:
        print(f"AI summarization failed, using fallback: {str(e)}")
        return fallback_summarize(text_limited)

def fallback_summarize(text):
    """Simple extractive summarization when AI fails"""
    # Split into sentences
    sentences = re.split(r'(?<=[.!?])\s+', text)

    # If very short text, return as is
    if len(sentences) <= 2:
        return text

    # Return first two sentences as summary
    return ' '.join(sentences[:2])

def extract_keywords(text):
    """Extract keywords with fallback if AI times out"""
    # Limit to 500 characters for better performance
    text_limited = text[:5000]

    prompt = f"""List 10 key terms from this text:

{text_limited}

Keywords:"""

    try:
        keywords = generate_text(prompt, max_tokens=1000)  # Reduced max tokens
        if len(keywords.strip()) < 5:  # If too short, use fallback
            return fallback_extract_keywords(text_limited)
        return keywords
    except Exception as e:
        print(f"AI keyword extraction failed, using fallback: {str(e)}")
        return fallback_extract_keywords(text_limited)

def fallback_extract_keywords(text):
    """Simple frequency-based keyword extraction when AI fails"""
    # Preprocess text
    processed_text = preprocess_text(text)

    # Count word frequencies
    words = processed_text.split()
    word_counts = {}
    for word in words:
        if len(word) > 3:  # Only consider words longer than 3 chars
            word_counts[word] = word_counts.get(word, 0) + 1

    # Sort by frequency
    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)

    # Take top 5 or fewer
    top_words = sorted_words[:5] if len(sorted_words) >= 5 else sorted_words

    # Format response
    result = "Keywords: " + ", ".join([word for word, _ in top_words])
    return result

# Helper function to get approximately 20 sentences from text
def get_preview_text(text, num_sentences=20):
    """Extract approximately 20 sentences from text for preview"""
    # Split text into sentences
    sentences = re.split(r'(?<=[.!?])\s+', text)

    # Take first num_sentences or all if less
    preview_sentences = sentences[:num_sentences] if len(sentences) > num_sentences else sentences

    # Join back into text
    preview = ' '.join(preview_sentences)

    # Add ellipsis if we truncated
    if len(sentences) > num_sentences:
        preview += '...'

    return preview

# Main functionality functions with optimize memory management - MODIFIED for longer previews

def check_similarity(file1, file2, progress=gr.Progress()):
    # Aggressive memory clearing
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    if file1 is None or file2 is None:
        return "Please upload both files to compare.", "", "", "", "", 0

    if not (allowed_file(os.path.basename(file1.name)) and allowed_file(os.path.basename(file2.name))):
        return "File type not allowed. Please upload TXT or PDF files.", "", "", "", "", 0

    try:
        # Extract text from files
        progress(0.2, desc="Extracting text from documents...")
        text1 = extract_text_from_file(file1)
        text2 = extract_text_from_file(file2)

        # Get similarity percentage
        progress(0.4, desc="Analyzing similarity...")
        start_time = time.time()
        similarity = compare_documents_with_ai(text1, text2)
        processing_time = time.time() - start_time

        progress(0.8, desc="Generating results...")

        # Return results
        result = f"## Similarity Analysis Complete\n\n**Similarity Score:** {similarity:.1f}%\n\n**Processing Time:** {processing_time:.2f} seconds"

        # Text preview (MODIFIED to show approximately 20 sentences)
        text1_preview = get_preview_text(text1, 20)
        text2_preview = get_preview_text(text2, 20)

        progress(1.0, desc="Done!")

        # Aggressive memory clearing again
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return result, text1_preview, text2_preview, os.path.basename(file1.name), os.path.basename(file2.name), similarity

    except Exception as e:
        error_msg = f"Error processing files: {str(e)}"
        print(error_msg)
        return error_msg, "", "", "", "", 0

def summarize_document(file, progress=gr.Progress()):
    # Aggressive memory clearing
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    if file is None:
        return "Please upload a file to summarize.", "", ""

    if not allowed_file(os.path.basename(file.name)):
        return "File type not allowed. Please upload TXT or PDF files.", "", ""

    try:
        # Extract text from file
        progress(0.3, desc="Extracting text from document...")
        text = extract_text_from_file(file)

        # Get summary
        progress(0.5, desc="Generating summary...")
        start_time = time.time()
        summary = summarize_text(text)
        processing_time = time.time() - start_time

        progress(0.8, desc="Formatting results...")

        # Return results
        result = f"## Document Summary\n\n{summary}\n\n**Processing Time:** {processing_time:.2f} seconds"

        # Text preview (MODIFIED to show approximately 20 sentences)
        text_preview = get_preview_text(text, 20)

        progress(1.0, desc="Done!")

        # Aggressive memory clearing again
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return result, text_preview, os.path.basename(file.name)

    except Exception as e:
        error_msg = f"Error processing file: {str(e)}"
        print(error_msg)
        return error_msg, "", ""

def extract_document_keywords(file, progress=gr.Progress()):
    # Aggressive memory clearing
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    if file is None:
        return "Please upload a file to extract keywords.", "", ""

    if not allowed_file(os.path.basename(file.name)):
        return "File type not allowed. Please upload TXT or PDF files.", "", ""

    try:
        # Extract text from file
        progress(0.3, desc="Extracting text from document...")
        text = extract_text_from_file(file)

        # Get keywords
        progress(0.5, desc="Identifying keywords...")
        start_time = time.time()
        keywords = extract_keywords(text)
        processing_time = time.time() - start_time

        progress(0.8, desc="Formatting results...")

        # Return results
        result = f"## Document Keywords\n\n{keywords}\n\n**Processing Time:** {processing_time:.2f} seconds"

        # Text preview (MODIFIED to show approximately 20 sentences)
        text_preview = get_preview_text(text, 20)

        progress(1.0, desc="Done!")

        # Aggressive memory clearing again
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return result, text_preview, os.path.basename(file.name)

    except Exception as e:
        error_msg = f"Error processing file: {str(e)}"
        print(error_msg)
        return error_msg, "", ""

# Define custom CSS for improved aesthetics
custom_css = """
.container {
    max-width: 1200px;
    margin-left: auto;
    margin-right: auto;
    padding-top: 1.5rem;
}

.header {
    text-align: center;
    margin-bottom: 2rem;
}

.header h1 {
    font-size: 2.5rem;
    font-weight: 700;
    margin-bottom: 0.5rem;
    background: linear-gradient(90deg, #4776E6 0%, #8E54E9 100%);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
}

.header p {
    font-size: 1.1rem;
    color: #718096;
    margin-bottom: 1.5rem;
}

.tab-content {
    padding: 1.5rem;
    border-radius: 0.5rem;
    background-color: #f8fafc;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
}

.result-box {
    border-radius: 0.5rem;
    border: 1px solid #e2e8f0;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
}

.file-upload {
    border: 2px dashed #cbd5e0;
    border-radius: 0.5rem;
    padding: 2rem;
    text-align: center;
    transition: all 0.3s ease;
}

.file-upload:hover {
    border-color: #4299e1;
    background-color: #ebf8ff;
}

.action-button {
    background: linear-gradient(90deg, #4776E6 0%, #8E54E9 100%);
    border: none;
    color: white;
    padding: 0.75rem 1.5rem;
    border-radius: 0.5rem;
    font-weight: 600;
    transition: all 0.3s ease;
}

.action-button:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(66, 153, 225, 0.3);
}

.similarity-meter {
    height: 2rem;
    border-radius: 1rem;
    overflow: hidden;
    background-color: #edf2f7;
    margin-bottom: 1rem;
}

.similarity-progress {
    height: 100%;
    background: linear-gradient(90deg, #48bb78 0%, #f6e05e 50%, #f56565 100%);
    transition: width 0.3s ease;
}

.panel {
    border-radius: 0.5rem;
    background-color: white;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
    padding: 1.5rem;
    margin-bottom: 1.5rem;
}

.footer {
    text-align: center;
    margin-top: 2rem;
    padding: 1.5rem;
    color: #718096;
    font-size: 0.875rem;
}

.gradio-container {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
}
"""

# Create the Gradio interface with enhanced UI
with gr.Blocks(title="Document Analysis Pro", theme=gr.themes.Soft(), css=custom_css) as app:
    # Header section
    with gr.Column(elem_classes="header"):
        gr.Markdown("# Document Analysis Pro")
        gr.Markdown("Analyze documents with AI-powered similarity detection, summarization, and keyword extraction.")
        gr.Markdown("*Powered by Granite 3.3 2B model from IBM*")

    # Main Tabs
    with gr.Tabs() as tabs:
        # Similarity Checker Tab
        with gr.Tab("üìä Similarity Checker", elem_id="similarity-tab"):
            with gr.Column(elem_classes="panel"):
                gr.Markdown("### Document Similarity Analysis")
                gr.Markdown("Upload two documents to compare their content and detect potential plagiarism.")

                with gr.Row(equal_height=True):
                    with gr.Column():
                        file1_input = gr.File(label="Document 1", elem_classes="file-upload")
                        file1_name = gr.Textbox(label="Filename", visible=False)

                    with gr.Column():
                        file2_input = gr.File(label="Document 2", elem_classes="file-upload")
                        file2_name = gr.Textbox(label="Filename", visible=False)

                similarity_btn = gr.Button("Analyze Similarity", variant="primary", elem_classes="action-button")

                # Visual similarity meter
                similarity_value = gr.Number(visible=False)
                with gr.Column():
                    gr.Markdown("### Similarity Score")
                    with gr.Row():
                        similarity_meter = gr.HTML(value="<div class='similarity-meter'><div class='similarity-progress' style='width:0%'></div></div>", visible=True)

                similarity_output = gr.Markdown(elem_classes="result-box")

                with gr.Accordion("Document Previews", open=False):
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("**Document 1**")
                            file1_preview = gr.Textbox(lines=12, label="")
                        with gr.Column():
                            gr.Markdown("**Document 2**")
                            file2_preview = gr.Textbox(lines=12, label="")

        # Document Summarizer Tab
        with gr.Tab("üìù Document Summarizer", elem_id="summary-tab"):
            with gr.Column(elem_classes="panel"):
                gr.Markdown("### AI Document Summarization")
                gr.Markdown("Upload a document to generate a concise, intelligent summary.")

                summary_file_input = gr.File(label="Upload Document", elem_classes="file-upload")
                summary_file_name = gr.Textbox(label="Filename", visible=False)

                summary_btn = gr.Button("Generate Summary", variant="primary", elem_classes="action-button")

                summary_output = gr.Markdown(elem_classes="result-box")

                with gr.Accordion("Document Preview", open=False):
                    summary_file_preview = gr.Textbox(lines=12, label="")

        # Keyword Extractor Tab
        with gr.Tab("üîç Keyword Extractor", elem_id="keyword-tab"):
            with gr.Column(elem_classes="panel"):
                gr.Markdown("### AI Keyword Extraction")
                gr.Markdown("Upload a document to identify key terms, topics, and concepts.")

                keyword_file_input = gr.File(label="Upload Document", elem_classes="file-upload")
                keyword_file_name = gr.Textbox(label="Filename", visible=False)

                keyword_btn = gr.Button("Extract Keywords", variant="primary", elem_classes="action-button")

                keyword_output = gr.Markdown(elem_classes="result-box")

                with gr.Accordion("Document Preview", open=False):
                    keyword_file_preview = gr.Textbox(lines=12, label="")

    # About section
    with gr.Accordion("‚ÑπÔ∏è About This Tool", open=False, elem_classes="panel"):
        gr.Markdown("""
        ## About Document Analysis Pro

        This professional document analysis application leverages advanced AI models to provide comprehensive text analysis capabilities:

        ### Key Features

        - **Similarity Analysis**: Compare documents and detect potential plagiarism with precise similarity scoring
        - **Intelligent Summarization**: Generate concise, contextually relevant summaries of complex documents
        - **Keyword Extraction**: Identify the most important terms and concepts within your documents

        ### Technology

        - Built on IBM's Granite 3.3 2B model with advanced NLP capabilities
        - Optimized for performance on various hardware configurations
        - Designed with memory efficiency for reliable operation

        ### Supported Formats

        - PDF documents
        - Plain text files (.txt)

        ### Performance Notes

        - Processing time varies based on document length and complexity
        - For best results, ensure documents are properly formatted and contain clear text
        - The application uses fallback mechanisms to ensure reliable operation even with resource constraints

        *For technical support or to report issues, please contact the development team.*
        """)

    # Footer
    with gr.Row(elem_classes="footer"):
        gr.Markdown("¬© 2025 Document Analysis Pro | Optimized for speed and accuracy")

    # Set up event handlers
    similarity_btn.click(
        fn=check_similarity,
        inputs=[file1_input, file2_input],
        outputs=[similarity_output, file1_preview, file2_preview, file1_name, file2_name, similarity_value]
    )

    # Update similarity meter based on similarity value
    similarity_value.change(
        fn=lambda value: f"<div class='similarity-meter'><div class='similarity-progress' style='width:{value}%'></div></div>",
        inputs=[similarity_value],
        outputs=[similarity_meter]
    )

    summary_btn.click(
        fn=summarize_document,
        inputs=[summary_file_input],
        outputs=[summary_output, summary_file_preview, summary_file_name]
    )

    keyword_btn.click(
        fn=extract_document_keywords,
        inputs=[keyword_file_input],
        outputs=[keyword_output, keyword_file_preview, keyword_file_name]
    )

    # Add JavaScript for additional interactivity
    app.load(js="""
    function updateTabStyles() {
        // Add highlight to active tab
        const tabs = document.querySelectorAll('.tabs > .tabitem');
        tabs.forEach(tab => {
            tab.addEventListener('click', function() {
                tabs.forEach(t => t.classList.remove('active-tab'));
                this.classList.add('active-tab');
            });
        });
    }

    // Initial setup when page loads
    document.addEventListener('DOMContentLoaded', function() {
        updateTabStyles();
    });
    """)

# Define function to pre-load the model (optional)
def preload_model():
    print("Pre-loading model. This may take a minute...")
    init_model()
    print("Model pre-loaded successfully")

# Launch with share=True for public URL
print("Starting Gradio interface...")
app.launch(share=True)

#Uncomment this to preload the model
#preload_model()